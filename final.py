# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.


## **Thematic Map Generator for Land Cover** ##

___
# Mount to my Google Drive
"""

#Mount to my Google Drive
from google.colab import drive
drive.mount('/gdrive')

!nvidia-smi

"""___
#Unzip data and make validation dataset
"""

#Unzip map data
#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/train/[원천]1.항공사진_Fine_512픽셀.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/train/image'
#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/train/[원천]2.항공사진_Fine_1024픽셀.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/train/image'
#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/train/[원천]3.위성영상_Fine_512픽셀.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/train/image'

#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/train/[라벨]위성_512_1.Ground_Truth_Tiff.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/train/label'
#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/train/[라벨]항공_512_1.Ground_Truth_Tiff.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/train/label'
#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/train/[라벨]항공_1024_1.Ground_Truth_Tiff.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/train/label'

#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/test/[원천]1.항공사진_Fine_512픽셀.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/test/image'
#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/test/[원천]2.항공사진_Fine_1024픽셀.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/test/image'
#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/test/[원천]3.위성영상_Fine_512픽셀.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/test/image'

#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/test/[라벨]위성_512_1.Ground_Truth_Tiff.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/test/label'
#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/test/[라벨]항공_512_1.Ground_Truth_Tiff.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/test/label'
#!unzip '/gdrive/My Drive/Colab Notebooks/final/data/test/[라벨]항공_1024_1.Ground_Truth_Tiff.zip' -d '/gdrive/My Drive/Colab Notebooks/final/data/test/label'


#Make validation set
#import shutil
#img_list = sorted([name for name in listdir('/gdrive/My Drive/Colab Notebooks/final/data/train/image') if isfile(join('/gdrive/My Drive/Colab Notebooks/final/data/train/image', name))])
#lbl_list = sorted([name for name in listdir('/gdrive/My Drive/Colab Notebooks/final/data/train/label') if isfile(join('/gdrive/My Drive/Colab Notebooks/final/data/train/label', name))])
#idx_list = [x for x in range(len(img_list))]
#val_idx_list = random.sample(idx_list, 2060)
#for idx in range(2060):
#  shutil.move('/gdrive/My Drive/Colab Notebooks/final/data/train/image/'+img_list[idx],'/gdrive/My Drive/Colab Notebooks/final/data/validation/image')
#  shutil.move('/gdrive/My Drive/Colab Notebooks/final/data/train/label/'+lbl_list[idx],'/gdrive/My Drive/Colab Notebooks/final/data/validation/label')

"""___
#Import libraries
"""

import os
from os import listdir
from os.path import isfile, join
import time
import logging
import random
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter
from torch.optim import SGD

from torchvision import transforms
from torchvision.utils import make_grid

"""___
#Define global variables
"""

#Define global variables
root = '/gdrive/My Drive/Colab Notebooks/final'
device = 'cuda' if torch.cuda.is_available() else 'cpu'

"""___
#Define custom dataset class

"""

#Give tesnored map data
class TensorMapDataset(Dataset):
    def __init__(self, img_path, lbl_path, img_size=(216, 216), aug=True, img_transform=None, lbl_transform=None):
        self.img_path = img_path
        self.lbl_path = lbl_path
        self.img_transform = img_transform
        self.lbl_transform = lbl_transform
        self.img_list = sorted([name for name in listdir(img_path) if isfile(join(img_path, name))])
        self.lbl_list = sorted([name for name in listdir(lbl_path) if isfile(join(lbl_path, name))])
        self.img_size = img_size
        self.aug = aug

    def __len__(self):
        return len(self.img_list)

    def __getitem__(self, idx):
        img_size = self.img_size
        img = Image.open(join(self.img_path, self.img_list[idx]))
        lbl = Image.open(join(self.lbl_path, self.lbl_list[idx]))
        img = transforms.ToTensor()(img) #change values to floats between [0,1]
        lbl = transforms.PILToTensor()(lbl) #maintain original values
        
        #ignore NIR channel
        #take only R,G,B channels for a image
        if(img.size(dim=0) > 3):
            img = img[:-(img.size(dim=0)-3)]

        #ignore NIR channel
        #take only R,G,B channels
        if(lbl.size(dim=0) > 1):
            lbl = lbl[:-(lbl.size(dim=0)-1)]
        
        #crop image to img_size
        crop_params = transforms.RandomCrop(img_size).get_params(img, img_size)
        img = transforms.functional.crop(img, *crop_params)
        lbl = transforms.functional.crop(lbl, *crop_params)
        
        if(self.aug == True):
            #random horizontal flip
            p = torch.rand(1)
            p = 1.0 if p > 0.5 else 0.0
            img = transforms.RandomHorizontalFlip(p)(img)
            lbl = transforms.RandomHorizontalFlip(p)(lbl)

            #random vertical flip
            p = torch.rand(1)
            p = 1.0 if p > 0.5 else 0.0
            img = transforms.RandomVerticalFlip(p)(img)
            lbl = transforms.RandomVerticalFlip(p)(lbl)

        #apply transforms
        if self.img_transform:
            img = self.img_transform(img)
        if self.lbl_transform:
            lbl = self.lbl_transform(lbl)
        return img, lbl

"""___
#Find mean and standard deviation for the dataset
"""

def getMeanAndStdv(img_path):
    dummy = img_path
    data = TensorMapDataset(img_path, dummy, (512, 512), False)
    dataloader = DataLoader(data, batch_size = 256, shuffle = False, num_workers=2)
    mean, stdv = 0, 0
    for bn, (imgs, _) in enumerate(dataloader):
        #caculate mean and stdv for each channel of an image, and then sum them individually
        print("processing batch number: ", bn)
        imgs = imgs.view(imgs.size(dim=0), imgs.size(dim=1), -1)
        mean += imgs.mean(dim=2).sum(dim=0)
        stdv += imgs.std(dim=2).sum(dim=0)
    print("done")
    img_num = len(dataloader.dataset)
    #return mean of image means and mean of image stdvs
    return mean/img_num, stdv/img_num 

#Get mean and stdv of train images
#mean, stdv = getMeanAndStdv(root+'/data/train/image')
#print(mean, stdv)

#Result of getMeanAndStdv function (it is performed on 512x512 images)
mean, stdv = [0.3418, 0.4022, 0.3525],[0.1593, 0.1435, 0.1242]

"""___
#Define data transforms
"""

#Define data variable
batch_size = 12
class_num = 9

#Define image transform
img_transform = transforms.Compose([
    transforms.Normalize(mean, stdv)
])

#Adjust class values from 10, 20, ..., 80, 100 to 0, 1, 2, ...,7, 8
class adjustClassValue:
    def __call__(self, lbl):
        output = torch.div(lbl, class_num+2, rounding_mode='trunc') #adjust class value to integer in 0, 1, ..., 7, 9
        output[output == 9] = 8 #adjust last class value 9 to 8
        return output

#Convert element type to long
class toLongTensor:
    def __call__(self, lbl):
        output = lbl.long()
        return output

#Define label transform
lbl_transform = transforms.Compose([
    adjustClassValue(),
    toLongTensor()
])

"""___
#Define datasets and dataloaders
"""

#Define datasets
train_dataset = TensorMapDataset(root+'/data/train/image', root+'/data/train/label', (216, 216), True, img_transform, lbl_transform)
validation_dataset = TensorMapDataset(root+'/data/validation/image', root+'/data/validation/label', (216, 216), True, img_transform, lbl_transform)
test_dataset = TensorMapDataset(root+'/data/test/image', root+'/data/test/label', (512, 512), False, img_transform, lbl_transform)

#Define dataloaders
train_dataloader = DataLoader(train_dataset, batch_size, shuffle = True, num_workers=4)
validation_dataloader = DataLoader(validation_dataset, batch_size, shuffle = False, num_workers=4)
test_dataloader = DataLoader(test_dataset, batch_size, shuffle = False, num_workers=4)

"""___
#HRNet

Define HRNet components
"""

#Define a loger
logger = logging.getLogger('hrnet_backbone')

#Define 3x3 convolution which does not reduce resolution when stride = 1
def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=dilation, groups=groups, bias=False, dilation=dilation)

#Define 1x1 convolution
def conv1x1(in_planes, out_planes, stride=1):
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)

"""Define BasicBlock"""

#Define Basic Block of HRNet
class BasicBlock(nn.Module):
    #expands the number of out channels to planes*expansion
    expansion = 1

    def __init__(self, in_channels, out_channels, stride=1, groups=1, base_width=64, dilation=1, norm_layer=None):
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError('BasicBlock only supports groups=1 and base_width=64')
        if dilation > 1:
            raise NotImplementedError("Dilation > 1 not supported in BasicBlock")
        
        width = out_channels #intermediate number of channels in this block
        self.conv1 = conv3x3(in_channels, width, stride)
        self.bn1 = norm_layer(width)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(width, out_channels*self.expansion)
        self.bn2 = norm_layer(out_channels*self.expansion)
        self.stride = stride

        #if stride is bigger than 1 or if the number of in channels and out channels are diffrent, 
        #block must adjust its input x to match its feature map for residual connection
        if self.stride != 1 or in_channels != out_channels*self.expansion:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels*self.expansion, kernel_size=1, stride=stride, bias=False),
                norm_layer(out_channels*self.expansion)
            )
        else:
            self.downsample = None

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        #when the number of channels of x and out are not same,
        #downsample is provided to match the number of channels of x and out
        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out

"""Define Bottleneck"""

#Define Bootleneck
class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, in_channels, out_channels, stride=1, groups=1, base_width=64, dilation=1, norm_layer=None):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        
        width = int(in_channels * (base_width / 64.)) * groups
        self.conv1 = conv1x1(in_channels, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv1x1(width, out_channels * self.expansion)
        self.bn3 = norm_layer(out_channels * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.stride = stride

        if self.stride != 1 or in_channels != out_channels*self.expansion:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels*self.expansion, kernel_size=1, stride=stride, bias=False),
                norm_layer(out_channels*self.expansion),
            )
        else:
            self.downsample = None

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out

"""Define HighResolutionModule

This moduel is a main module of HRNet
"""

#Define High Resolution Module of HRNet
class HighResolutionModule(nn.Module):
    def __init__(self, num_branches, block_type, num_blocks, num_inchannels,
                 num_channels, multi_scale_output=True, norm_layer=None):
        super(HighResolutionModule, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d

        self.num_branches = num_branches 
        self.block = block_type 
        self.num_blocks = num_blocks
        self.num_inchannels = num_inchannels 
        self.num_channels = num_channels 
        self.multi_scale_output = multi_scale_output 
        self.norm_layer = norm_layer

        self.actual_num_channels = [self.block.expansion*out_channels for out_channels in self.num_channels]
        self.relu = nn.ReLU(inplace=True)
        self.branches = self._make_branches()
        self.fuse_layers = self._make_fuse_layers()

    def _check_ok_to_make_branches(self):
        #the number of branches must be at least 1
        if self.num_branches < 1:
            error_msg = 'NUM_BRANCHES({}) must be at least 1'.format(
                self.num_branches)
            logger.error(error_msg)
            raise ValueError(error_msg)
        #the number of blocks for each branch must be provided
        if self.num_branches != len(self.num_blocks):
            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(
                self.num_branches, len(self.num_blocks))
            logger.error(error_msg)
            raise ValueError(error_msg)
        #the number of out channels for each branch must be provided
        if self.num_branches != len(self.num_channels):
            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(
                self.num_branches, len(self.num_channels))
            logger.error(error_msg)
            raise ValueError(error_msg)
        #the number of in channels for each branch must be provided
        if self.num_branches != len(self.num_inchannels):
            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(
                self.num_branches, len(self.num_inchannels))
            logger.error(error_msg)
            raise ValueError(error_msg)

    def _make_one_branch(self, branch_index):
        layers = []
        #append the first block of this branch which may reduce a resolution or may change the number of channels
        layers.append(self.block(self.num_inchannels[branch_index], self.num_channels[branch_index], norm_layer=self.norm_layer))
        #append remain blocks of this branch which maintain the resolution and the number of channels
        in_channels = self.num_channels[branch_index] * self.block.expansion
        for i in range(1, self.num_blocks[branch_index]):
            layers.append(self.block(in_channels, self.num_channels[branch_index], norm_layer=self.norm_layer))
        return nn.Sequential(*layers)

    def _make_branches(self):
        #parameters for each branch must be provided
        self._check_ok_to_make_branches()
        branches = []
        for branch_idx in range(self.num_branches):
            branches.append(self._make_one_branch(branch_idx))
        return nn.ModuleList(branches)

    #if multi_scale_output is True, then all branches exchange thier information with each other.
    #if False, then collect all information to the main branch(the branch whose resolution is most high) 
    def _make_fuse_layers(self):
        if self.num_branches == 1:
            return None
        
        num_branches = self.num_branches
        num_inchannels = self.actual_num_channels
        fuse_layers = []
        for i in range(num_branches if self.multi_scale_output else 1): #i is the current branch to collect information
            fuse_layer = []
            for j in range(num_branches):
                #for lower resolution branches, adjust output channel
                if j > i:
                    fuse_layer.append(
                        nn.Sequential(
                            nn.Conv2d( 
                                  num_inchannels[j],
                                  num_inchannels[i], 
                                  kernel_size=1, 
                                  stride=1,
                                  padding=0,
                                  bias=False
                                  ),
                            self.norm_layer(num_inchannels[i])
                            )
                        )
                #do not need to fuse itself
                elif j == i:
                    fuse_layer.append(None)
                #for higher resolution branches, adjust output channel and resolution
                else:
                    conv3x3s = []
                    for k in range(i-j): 
                        if k == i - j - 1: 
                            num_outchannels_conv3x3 = num_inchannels[i]
                            conv3x3s.append(
                              nn.Sequential(
                                  nn.Conv2d(
                                      num_inchannels[j],
                                      num_outchannels_conv3x3, 
                                      kernel_size=3, 
                                      stride=2, 
                                      padding=1, 
                                      bias=False
                                      ),
                                  self.norm_layer(num_outchannels_conv3x3)
                                  )
                              )
                        else:
                            num_outchannels_conv3x3 = num_inchannels[j]
                            conv3x3s.append(
                              nn.Sequential(
                                  nn.Conv2d(
                                      num_inchannels[j],
                                      num_outchannels_conv3x3, 
                                      kernel_size=3, 
                                      stride=2, 
                                      padding=1, 
                                      bias=False
                                      ),
                                  self.norm_layer(num_outchannels_conv3x3),
                                  nn.ReLU(inplace=True)
                                  )
                              )
                    fuse_layer.append(nn.Sequential(*conv3x3s))
            #add the fuse layer of the current branch i
            fuse_layers.append(nn.ModuleList(fuse_layer))

        return nn.ModuleList(fuse_layers)

    def get_actual_num_outchannels(self):
        return self.actual_num_channels

    #return a list which contains fused feature maps of each branch
    def forward(self, x): #x contains inputs for each branch
        if self.num_branches == 1:
            return [self.branches[0](x[0])]

        for i in range(self.num_branches):
            x[i] = self.branches[i](x[i])

        x_fuse = []
        for i in range(len(self.fuse_layers)):
            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])
            for j in range(1, self.num_branches):
                #add itself
                if i == j:
                    y = y + x[j]
                #fuse the branches which have lower resolution
                elif j > i:
                    width_output = x[i].shape[-1]
                    height_output = x[i].shape[-2]
                    y = y + F.interpolate(
                        self.fuse_layers[i][j](x[j]),
                        size=[height_output, width_output], #upsampling
                        mode='bilinear',
                        align_corners=True #give better performance for semantic segmentation
                        )
                #fuse the branches which have higher resolution
                else:
                    y = y + self.fuse_layers[i][j](x[j]) 
            x_fuse.append(self.relu(y))

        return x_fuse

"""Define block type

It is used to dictate a block type in a module in HRNet
"""

#Define Block types
blocks_dict = {
    'BASIC': BasicBlock,
    'BOTTLENECK': Bottleneck
}

"""Define HRNet"""

#Define HRNet
class HighResolutionNet(nn.Module):

    def __init__(self, cfg, class_num, norm_layer=None):
        super(HighResolutionNet, self).__init__()

        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self.norm_layer = norm_layer

        #stem net
        init_channel = cfg['INIT_CHANNEL']
        self.conv1 = nn.Conv2d(3, init_channel, kernel_size=3, stride=1, padding=1,bias=False)
        self.bn1 = self.norm_layer(init_channel)
        self.conv2 = nn.Conv2d(init_channel, init_channel, kernel_size=3, stride=1, padding=1,bias=False)
        self.bn2 = self.norm_layer(init_channel)
        self.relu = nn.ReLU(inplace=True)
        self.cfg = cfg

        #make stages
        transitions = []
        stages = []
        pre_stage_channels = [init_channel]
        for offset in range(len(cfg)-1):
            stage_idx = 1 + offset
            stage_name = 'STAGE' + str(stage_idx)
            stage_cfg = cfg[stage_name]
            block = blocks_dict[stage_cfg['BLOCK']]
            num_blocks = stage_cfg['NUM_BLOCKS']
            num_channels = stage_cfg['NUM_CHANNELS']
            actual_num_outchannels = [out_channels * block.expansion for out_channels in num_channels]
            transition = self._make_transition_layer(pre_stage_channels, actual_num_outchannels)
            stage, pre_stage_channels = self._make_stage(stage_cfg, actual_num_outchannels)
            transitions.append(transition)
            stages.append(stage)
        self.transitions = nn.ModuleList(transitions)
        self.stages = nn.ModuleList(stages)

        #last net
        last_inp_channels = np.int(np.sum(pre_stage_channels))
        self.last_layer = nn.Sequential(
            nn.Conv2d(last_inp_channels, last_inp_channels, kernel_size=1,stride=1,padding=0),
            self.norm_layer(last_inp_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(last_inp_channels, class_num, kernel_size=1, stride=1,padding=0)
        )

    #transition layer is used to make inputs for each branch
    def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):
        num_branches_cur = len(num_channels_cur_layer)
        num_branches_pre = len(num_channels_pre_layer)

        transition_layers = []
        for i in range(num_branches_cur):
            #for old branches
            if i < num_branches_pre:
                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:
                    transition_layers.append(nn.Sequential(
                        nn.Conv2d(
                                  num_channels_pre_layer[i],
                                  num_channels_cur_layer[i], #maintain the number of channels
                                  kernel_size=3,
                                  stride=1,
                                  padding=1, #maintain resolution
                                  bias=False
                                  ),
                        self.norm_layer(num_channels_cur_layer[i]),
                        nn.ReLU(inplace=True)))
                else:
                    transition_layers.append(None)
            #for new born branches
            else:
                conv3x3s = []
                for j in range(i+1-num_branches_pre): #reduce resolution by half repeatedly
                    inchannels = num_channels_pre_layer[-1]
                    outchannels = num_channels_cur_layer[i] if j == i-num_branches_pre else inchannels
                    conv3x3s.append(nn.Sequential(
                        nn.Conv2d(
                                  inchannels, 
                                  outchannels, 
                                  kernel_size=3, 
                                  stride=2, #reduce resolution by half
                                  padding=1, 
                                  bias=False
                                  ), 
                        self.norm_layer(outchannels),
                        nn.ReLU(inplace=True)))
                transition_layers.append(nn.Sequential(*conv3x3s))

        return nn.ModuleList(transition_layers)

    def _make_stage(self, layer_config, num_inchannels, multi_scale_output=True):
        num_modules = layer_config['NUM_MODULES']
        num_branches = layer_config['NUM_BRANCHES']
        num_blocks = layer_config['NUM_BLOCKS']
        num_channels = layer_config['NUM_CHANNELS']
        block = blocks_dict[layer_config['BLOCK']]

        modules = []
        for i in range(num_modules):
            # multi_scale_output is only used at last module
            if not multi_scale_output and i == num_modules - 1:
                reset_multi_scale_output = False
            else:
                reset_multi_scale_output = True

            modules.append(
                HighResolutionModule(num_branches,
                                     block,
                                     num_blocks,
                                     num_inchannels,
                                     num_channels,
                                     reset_multi_scale_output,
                                     norm_layer=self.norm_layer)
            )
            num_inchannels = modules[-1].get_actual_num_outchannels() #the actual number of out channels of this stage

        return nn.Sequential(*modules), num_inchannels

    def forward(self, x):
        h, w = x.size(dim=2), x.size(dim=3)

        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)

        x = [x]
        for stage_idx, stage in enumerate(self.stages):
            xs_for_branches = []
            for branch_idx, transition in enumerate(self.transitions[stage_idx]):
                if transition is not None:
                    if branch_idx < len(x):
                        xs_for_branches.append(transition(x[branch_idx]))
                    else:
                        xs_for_branches.append(transition(x[-1])) #new born branch uses the output of last old branch
                else:
                    xs_for_branches.append(x[branch_idx])
            x = stage(xs_for_branches)

        # Upsampling
        x_list = []
        for idx in range(len(x)):
            x_list.append(F.interpolate(x[idx], size=(h, w), mode='bilinear', align_corners=True))
        x = torch.cat(x_list, dim=1)
        out = self.last_layer(x)

        return out

"""___
#Define helper function for training
"""

#Get counts of 0 to n_class**2 - 1
#Only counts in diagnoal are valid
def _fast_hist(label_true, label_pred, n_class):
    mask = (label_true >= 0) & (label_true < n_class)
    hist = np.bincount(
        n_class * label_true[mask].astype(int) +
        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)
    return hist

#Get evalution metrics scores
def label_accuracy_score(label_trues, label_preds, n_class):
    hist = np.zeros((n_class, n_class))
    for lt, lp in zip(label_trues, label_preds):
        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)
    acc = np.diag(hist).sum() / hist.sum()
    with np.errstate(divide='ignore', invalid='ignore'):
        iou = np.diag(hist) / (
            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)
        )
    mean_iou = np.nanmean(iou)
    return acc, mean_iou

#Restore class number in 10, 20, ..., 80, 100
def restore(lbl):
    lbl[lbl == 8] = 9
    output = torch.mul(torch.add(lbl, 1), 10)
    return output

#Get batch index and image index in batch for sample images 
def getSampleIdxs(dataloader):
    #get random image indexes
    return random.randint(0,len(dataloader))
        
#Get denormalize images and predictions by the model
def getUnNormAndPred(sample_images):
    b, _, h, w = sample_images.size()
    permuted_images = sample_images.permute(1,0,2,3)
    denorm_images = torch.zeros_like(permuted_images)
    for idx, (channel_image, m, s) in enumerate(zip(permuted_images, mean, stdv)):
            denorm_images[idx] = (channel_image * s) + m
    denorm_images = denorm_images.permute(1,0,2,3)
    with torch.no_grad():
        output = model(sample_images.to(device))
        preds = output.argmax(dim=1).view(b, 1, h, w)
    return denorm_images, preds

#Visualize sample images, predictions, and labels in Tensor Board
def visualizeImgInTB(mode, epoch, sample_images, sample_labels):
    #image visualization 
    denorm_images, preds = getUnNormAndPred(sample_images)

    #stitch images into a grid.
    tb_denorm = make_grid(denorm_images, nrow=4)
    tb_label = make_grid(restore(sample_labels), nrow=4)
    tb_pred = make_grid(restore(preds), nrow=4)

    #write images to tensorboard.
    writer.add_image(mode+'_pred', tb_pred, epoch)
    writer.add_image(mode+'_gt', tb_label, epoch)
    writer.add_image(mode+'_img', tb_denorm, epoch)
    
    return

#Write model results to Tensor Board
def writeResultInTB(mode, dataloader, epoch, total_loss, total_ious, total_pixel_acc):
    writer.add_scalar(mode+'_loss', total_loss / len(validation_dataloader), epoch)
    writer.add_scalar(mode+'_pixel_acc', total_pixel_acc, epoch)
    writer.add_scalar(mode+'_mean_iou', total_ious, epoch)

"""___
#Configure HRNet
"""

#Define model
cfg = {}

cfg['INIT_CHANNEL'] = 80

cfg['STAGE1'] = {}
cfg['STAGE1']['NUM_MODULES'] = 1
cfg['STAGE1']['NUM_BRANCHES'] = 2
cfg['STAGE1']['NUM_BLOCKS'] = [2, 4]
cfg['STAGE1']['NUM_CHANNELS'] = [40, 80]
cfg['STAGE1']['BLOCK'] = 'BOTTLENECK'

cfg['STAGE2'] = {}
cfg['STAGE2']['NUM_MODULES'] = 1
cfg['STAGE2']['NUM_BRANCHES'] = 3
cfg['STAGE2']['NUM_BLOCKS'] = [2, 2, 4]
cfg['STAGE2']['NUM_CHANNELS'] = [20, 40, 80]
cfg['STAGE2']['BLOCK'] = 'BASIC'

cfg['STAGE3'] = {}
cfg['STAGE3']['NUM_MODULES'] = 1
cfg['STAGE3']['NUM_BRANCHES'] = 4
cfg['STAGE3']['NUM_BLOCKS'] = [2, 2, 2, 4]
cfg['STAGE3']['NUM_CHANNELS'] = [10, 20, 40, 80]
cfg['STAGE3']['BLOCK'] = 'BASIC'

cfg['STAGE4'] = {}
cfg['STAGE4']['NUM_MODULES'] = 1
cfg['STAGE4']['NUM_BRANCHES'] = 4
cfg['STAGE4']['NUM_BLOCKS'] = [2, 2, 2, 2]
cfg['STAGE4']['NUM_CHANNELS'] = [10, 20, 40, 80]
cfg['STAGE4']['BLOCK'] = 'BASIC'

cfg['STAGE5'] = {}
cfg['STAGE5']['NUM_MODULES'] = 1
cfg['STAGE5']['NUM_BRANCHES'] = 4
cfg['STAGE5']['NUM_BLOCKS'] = [2, 2, 2, 2]
cfg['STAGE5']['NUM_CHANNELS'] = [5, 10, 20, 40]
cfg['STAGE5']['BLOCK'] = 'BASIC'

"""___
#Define variables for training
"""

#Set several data paths
num_trial=2
result_dir= root + '/results'
parent_dir = result_dir + f'/trial_{num_trial}'
print(f'Logs and ckpts will be saved in : {parent_dir}')
log_dir = parent_dir
ckpt_dir = parent_dir
ckpt_path = parent_dir + '/model.pt'

#Set a Writer for Tensorboard
writer = SummaryWriter(log_dir)

#move model to gpu
model = HighResolutionNet(cfg, class_num).to(device)

#Define optimizer
init_lr = 0.002
final_lr = 0.0005
optimizer = SGD(model.parameters(),lr=init_lr, momentum=0.9, weight_decay=0.0005)

#Define criterion
criterion = nn.CrossEntropyLoss(ignore_index=23) #ignore labelling error value(255 -> 23)

#Define sample index
sample_idx = getSampleIdxs(validation_dataloader)

"""___
#Setup Tensorboard
"""

# Commented out IPython magic to ensure Python compatibility.
#Setup tensorboard.
# %load_ext tensorboard
# %tensorboard --logdir "/gdrive/My Drive/{str(result_dir).replace('/gdrive/My Drive/', '')}"

"""___
#Define train function
"""

def train_net(epochs, resume=False):
    net = model
    best_valid_iou = 0

    if resume:
        checkpoint = torch.load(ckpt_path)
        net.load_state_dict(checkpoint['state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        epoch = checkpoint['epoch']
        best_valid_iou = checkpoint['best_valid_iou']
        print(f'Resume training from epoch {epoch+1}')
    else:
        epoch = 0

    sample_images, sample_labels = None, None

    while epoch < epochs:
        t1 = time.time()

        #use polynomial learning rate decay
        optimizer.param_groups[0]['lr'] = final_lr + (init_lr - final_lr) * ((1 - (epoch)/(epochs))**0.9) 

        #start training
        net.train()

        loss_total = 0
        ious = []
        pixel_accs = []

        for batch_idx, (image, label) in enumerate(train_dataloader):
            #save images for tensor board visualization.
            if batch_idx == sample_idx:
                sample_images = image.cpu()
                sample_labels = label.cpu()

            #move to gpu
            image = image.to(device)
            label = label.squeeze(dim=1).to(device)
            
            #train
            output = net(image)
            loss = criterion(output, label)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            pred = output.argmax(dim=1).cpu().detach().numpy()
            target = label.cpu().numpy()

            acc, mean_iu = label_accuracy_score(target, pred, n_class=class_num)
            loss_total += loss.item()
            pixel_accs.append(acc)
            ious.append(mean_iu)

            if batch_idx % 10 == 0:
                print(f'Epoch : {epoch+1} || {batch_idx}/{len(train_dataloader)} || loss : {loss.item():.3f} || iou : {mean_iu * 100:.3f} || pixel_acc : {acc * 100:.3f}')

        #save checkpoints every epoch.
        checkpoint = {
            'epoch': epoch + 1,
            'state_dict': net.state_dict(),
            'optimizer': optimizer.state_dict(),
            'best_valid_iou': best_valid_iou
        }
        torch.save(checkpoint, ckpt_path)
        
        #aggregate results
        total_ious = np.array(ious).T
        total_ious = np.nanmean(total_ious).mean()
        total_pixel_acc = np.array(pixel_accs).mean()

        #write results in Tensor Board
        writeResultInTB('train', train_dataloader, epoch, loss_total, total_ious, total_pixel_acc)
        visualizeImgInTB('train', epoch, sample_images, sample_labels)

        #print 1 epoch train result
        t = time.time() - t1
        print(f'>> Epoch : {epoch+1} || AVG loss : {loss_total / len(train_dataloader):.3f} || iou : {total_ious * 100:.3f} || pixel_acc : {total_pixel_acc * 100:.3f} || {t:.3f} secs')

        #validation
        net.eval()

        valid_loss_total = 0
        valid_ious = []
        valid_pixel_accs = []

        with torch.no_grad():
            for batch_idx, (image, label) in enumerate(validation_dataloader):
                #save images for visualization.
                if batch_idx == sample_idx:
                    sample_images = image.cpu()
                    sample_labels = label.cpu()
                
                image = image.to(device)
                label = label.squeeze(dim=1).to(device)
                output = net(image)
                loss = criterion(output, label)

                pred = output.argmax(dim=1).cpu().numpy()
                target = label.cpu().numpy()

                acc, mean_iu = label_accuracy_score(target, pred, n_class=class_num)

                valid_loss_total += loss.item()
                valid_pixel_accs.append(acc)
                valid_ious.append(mean_iu)

        #aggregate results
        total_valid_ious = np.array(valid_ious).T
        total_valid_ious = np.nanmean(total_valid_ious).mean()
        total_valid_pixel_acc = np.array(valid_pixel_accs).mean()

        #write results in Tensor Board
        writeResultInTB('valid', validation_dataloader, epoch, valid_loss_total, total_valid_ious, total_valid_pixel_acc)
        visualizeImgInTB('validation', epoch, sample_images, sample_labels)

        print(f'>> Epoch : {epoch+1} || AVG valid loss : {valid_loss_total / len(validation_dataloader):.3f} || iou : {total_valid_ious * 100:.3f} || pixel_acc : {total_valid_pixel_acc * 100:.3f} || {t:.3f} secs')

        #save best checkpoint.
        if total_valid_ious > best_valid_iou:
            best_valid_iou = total_valid_ious
            torch.save(net.state_dict(), ckpt_dir+'/best.pt')

        #update epoch
        epoch += 1
    
    #train of 1 epoch ends
    print(f'>> Best validation set iou: {best_valid_iou}')

"""___
#Do training
"""

train_net(epochs=200, resume=True)

"""___
#Define test function
"""

def test_net(path):
    best_path = path + '/best.pt'
    
    # do test
    net = model
    net.load_state_dict(torch.load(best_path))
    net.eval()

    valid_loss_total = 0
    valid_ious = []
    valid_pixel_accs = []
    valid_ious_crf = []
    valid_pixel_accs_crf = []

    sample_idx = [31, 33] #fix sample idx for arbitary number
    sample_images, sample_labels = [], []

    with torch.no_grad():
        for batch_idx, (image, label) in enumerate(test_dataloader):
            if batch_idx in sample_idx:
                sample_images.append(image.cpu())
                sample_labels.append(label.cpu())

            image = image.to(device)
            label = label.squeeze(dim=1).to(device)
            output = net(image)
            loss = criterion(output, label)

            pred = output.argmax(dim=1).cpu().numpy()
            target = label.cpu().numpy()

            acc, mean_iu = label_accuracy_score(target, pred, n_class=class_num)

            valid_loss_total += loss.item()
            valid_pixel_accs.append(acc)
            valid_ious.append(mean_iu)

        #calculate average IoU
        total_valid_ious = np.array(valid_ious).T
        total_valid_ious = np.nanmean(total_valid_ious).mean()
        total_valid_pixel_acc = np.array(valid_pixel_accs).mean()

        print(f'Pixel accuracy: {total_valid_pixel_acc * 100:.3f}, mIoU: {total_valid_ious:.3f}')

        visualizeImgInTB('test', 0, sample_images[0], sample_labels[0])
        visualizeImgInTB('test', 1, sample_images[1], sample_labels[1])

"""___
#Do testing
"""

test_net(result_dir + "/trial_" + str(num_trial))

